# @package _global_

defaults:
  - _self_
  - data: cifar10.yaml # mnist.yaml # imagenet.yaml # 
  - model: vit.yaml # cirsean4.yaml # conv_dense.yaml # general_torchvision.yaml # 
  - tracking: mlflow.yaml # set tracking here or use command line (e.g. `python train.py tracking=tensorboard`)
  - training: svd_trainer.yaml # qr_fix_trainer.yaml #
  - log_dir: default.yaml
  # experiment configs allow for version control of specific configurations
  # e.g. hyperparameters for each combination of model and data
  - experiment: null
  # debugging config (enable through command line, e.g. `python train.py debug=default)
  - debug: null
  # config for hyperparameter optimization
  - hparams_search: null
  # optional local config for machine/user specific settings
  # it's optional since it doesn't need to exist and is excluded from version control
  - optional local: default.yaml
  - override hydra/hydra_logging: colorlog
  - override hydra/job_logging: colorlog
# path to original working directory
# hydra hijacks working directory by changing it to the new log directory
# https://hydra.cc/docs/next/tutorials/basic/running_your_app/working_directory
original_work_dir: ${hydra:runtime.cwd}
# pretty print config at the start of the run using Rich library
print_config: True
# set False to skip model training
train: True
validate: True
test: False
seed: 42
# default name for the experiment in mlflow, determines the default logging folder path
# (you can overwrite this name in experiment configs)
name: "svd-reinitddp"
baseline: False

comm_method: "nccl"
cpu_training: False
enable_tracking: True



# Saving stuff ============================================
# save_weights: False
# save_frequency: 1
# save_layers: null
# save_parent_folder: /pfs/work7/workspace/scratch/qv2382-madonna/qv2382-madonna/saved_weights

# saving for cicarean:
# - local_model.model.0.weight
# - local_model.model.3.weight
# - local_model.model.6.weight
# - local_model.model.9.weight
# - local_model.model.12.weight
# - local_model.model.15.weight

# saving for ViT
# - encoder.layers.encoder_layer_0.self_attention.in_proj_weight
# - encoder.layers.encoder_layer_0.self_attention.out_proj.weight
# - encoder.layers.encoder_layer_0.mlp.0.weight
# - encoder.layers.encoder_layer_0.mlp.3.weight
# - encoder.layers.encoder_layer_2.self_attention.in_proj_weight
# - encoder.layers.encoder_layer_2.self_attention.out_proj.weight
# - encoder.layers.encoder_layer_2.mlp.0.weight
# - encoder.layers.encoder_layer_2.mlp.3.weight
# - encoder.layers.encoder_layer_5.self_attention.in_proj_weight
# - encoder.layers.encoder_layer_5.self_attention.out_proj.weight
# - encoder.layers.encoder_layer_5.mlp.0.weight
# - encoder.layers.encoder_layer_5.mlp.3.weight
# - heads.head.weight