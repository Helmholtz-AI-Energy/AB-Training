start_epoch: 0
epochs: 200

# number of validation steps to execute at the beginning of the training
# num_sanity_val_steps: 0
trainer: "ortho_fix_train"
init_ddp: True

# ckpt path
resume_from_checkpoint: null

criterion:
  _target_: torch.nn.CrossEntropyLoss
  label_smoothing: 0.1

lr: 0.0001
lr_schedule:
  _target_: torch.optim.lr_scheduler.StepLR #torch.optim.lr_scheduler.CosineAnnealingWarmRestarts # torch.optim.lr_scheduler.CosineAnnealingLR #
  _partial_: True
  step_size: 200
  gamma: 0.5
  # T_max: 25
  # T_0: 25
lr_warmup: #null
  _target_: pytorch_warmup.LinearWarmup
  _partial_: True
  warmup_period: 200

lr_warmup_step_frequency: "epoch"
lr_warmup_step_args: null

optimizer:
  # _target_: torch.optim.SGD
  # _partial_: True
  # momentum: 0.1
  # weight_decay: 1.0e-5
  # nesterov: True
  _target_: torch.optim.Adam
  _partial_: True

sigma_optimizer:  # -> optimizer specific for sigma - will not touch the other params
  _partial_: True
  _target_: torch.optim.Adam
  # _target_: torch.optim.SGD
  lr: 0.001
  # momentum: 0.9
  # weight_decay: 1.0e-5
  # nesterov: True
sigma_warmup: #null
  _target_: pytorch_warmup.LinearWarmup
  _partial_: True
  warmup_period: 10

init_opt_with_model: False
print_freq: 10

svd_epoch_delay: 12
fixing_method:
  _target_: madonna.models.SVDFixingModel
  _partial_: True
  stability_frequency: 100
  delay: 200
  uvhthreshold: 0.99
  sigma_cutoff_fraction: 0.2
  sync_usv: False
  full_rank_sigma: True
  keep_first_layer: False
  keep_last_layer: True
  update_from_simga: True
  reinit_shapes: True
