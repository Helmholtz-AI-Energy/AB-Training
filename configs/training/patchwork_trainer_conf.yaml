start_epoch: 0
epochs: 300
iterations_per_train: 100

# number of validation steps to execute at the beginning of the training
# num_sanity_val_steps: 0
trainer: "patchwork_trainer"

# ckpt path
checkpoint: null # /hkfs/work/workspace/scratch/qv2382-madonna-ddp/madonna/models/baseine/vit_b_16-imagenet/epoch43.pth.tar #
checkpoint_out_root: null  # /hkfs/work/workspace/scratch/qv2382-madonna-ddp/madonna/models
save_period: 25

resume_run: False
resume: False  # TODO: are both of these needed??

criterion:
  _target_: torch.nn.CrossEntropyLoss
  label_smoothing: 0.1

lr: 0.0001
lr_schedule:
  _target_: null
  # torch.optim.lr_scheduler.StepLR # torch.optim.lr_scheduler.CosineAnnealingWarmRestarts # torch.optim.lr_scheduler.CosineAnnealingLR #
  sched: cosine  # cosine LR scheduler from timm
  epochs: 300  # number of epochs for the cycle to run for
  warmup_epochs: 10
  cooldown_epochs: 10
  min_lr: 0.00001  # min LR of cosine annealing pattern
  warmup_lr: 0.00001  # starting point of warmup
  warmup_prefix: True  # default
  lr_cycle_mul: 1.5  # default -> cycle length: int(math.floor(-self.t_initial * (self.cycle_mul ** cycles - 1) / (1 - self.cycle_mul)))
  lr_cycle_limit: 30
  lr_cycle_decay: 0.4  # NOTE: the implementation is wrong in the docs!! it should be that the decay_rate is lr_cycle_decay!!!
  lr_k_decay: 1.0  # default 1.0
  sched_on_updates: True

sync_batchnorm: False

mixup: False
mixup_args:
  mixup_alpha: 0.8  # 0.8
  cutmix_alpha: 0.
  cutmix_minmax: null
  prob: 1.0  # 1.0
  switch_prob: 0.5
  mode: batch
  num_classes: null

optimizer:
  _partial_: True
  _target_: torch.optim.AdamW
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.999
max_grad_norm: -1.

p_optimizer: null
federated: False

init_opt_with_model: False
print_freq: 50

init_method: unified  # options: random, unified, random-sigma, ortho-sigma

patchwork_svd:
  warmup_steps: 10
  steps_btw_syncing: 10
  names_to_always_sync: null
  comm_method: all-to-all
  comm_kwargs:
    percent_to_send: 0.25
  cat1d: True
